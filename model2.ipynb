{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.0-beta"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x13e67e290> False\n<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13e67ebd0> False\n<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13e67ec10> False\n<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x13e6b9d10> False\n<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13e6ee3d0> False\n<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13e6f8f90> False\n<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x13e6ff590> False\n<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13e704cd0> False\n<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13e704490> False\n<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13e712c90> False\n<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x13e71cc10> False\n<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13e721ad0> False\n<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13e7509d0> False\n<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13e756e50> False\n<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x13e75be90> False\n<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13e75f790> True\n<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13e76b850> True\n<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x13e76b910> True\n<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x13e775c10> True\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nimage_input (InputLayer)     [(None, 150, 150, 3)]     0         \n_________________________________________________________________\nvgg16 (Model)                multiple                  14714688  \n_________________________________________________________________\nflatten (Flatten)            (None, 8192)              0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              33558528  \n_________________________________________________________________\nfc2 (Dense)                  (None, 1024)              4195328   \n_________________________________________________________________\npred (Dense)                 (None, 1)                 1025      \n=================================================================\nTotal params: 52,469,569\nTrainable params: 44,834,305\nNon-trainable params: 7,635,264\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "from tensorflow.python.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.python.keras.preprocessing import image\n",
    "from tensorflow.python.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.python.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.python.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "#Get back the convolutional part of a VGG network trained on ImageNet\n",
    "model_vgg16_conv = VGG16(weights='imagenet', include_top=False) # data_format = \"channels_first\"\n",
    "\n",
    "# Freeze the layers except the last 4 layers\n",
    "for layer in model_vgg16_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in model_vgg16_conv.layers:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    "#Create your own input format (here 3x200x200)\n",
    "input = Input(shape=(150,150,3),name = 'image_input')\n",
    "\n",
    "#Use the generated model \n",
    "output_vgg16_conv = model_vgg16_conv(input)\n",
    "\n",
    "#Add the fully-connected layers \n",
    "x = Flatten(name='flatten')(output_vgg16_conv)\n",
    "x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = Dense(1, activation='linear', name='pred')(x)\n",
    "\n",
    "#Create your own model \n",
    "my_model = Model(input, x)\n",
    "\n",
    "#In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
    "\n",
    "my_model.compile(loss='mse',\n",
    "              optimizer='adam',\n",
    "              metrics=['mse'])\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into dataframe with 'path' and 'target' columns\n",
    "import pandas as pd\n",
    "train_label_df = pd.read_csv('data.csv', delimiter=' ', header=1, names=[\"uid\"])\n",
    "train_label_df[\"path\"] = train_label_df['uid'].apply(lambda x : 'data/' + x.split(',')[-1][1:].split('/')[-1])\n",
    "train_label_df[\"target\"] = train_label_df['uid'].apply(lambda x : int(x.split(',')[1])/int(x.split(',')[2]))\n",
    "train_label_df =train_label_df.drop(['uid'], axis=1)\n",
    "\n",
    "train_label_df2 = pd.read_csv('data_temp.csv', delimiter=' ', header=1, names=[\"uid\"])\n",
    "train_label_df2[\"path\"] = train_label_df2['uid'].apply(lambda x : \"data/\" + x.split(',')[0] + \".jpg\")\n",
    "train_label_df2[\"target\"] = train_label_df2['uid'].apply(lambda x : int(x.split(',')[1])/int(x.split(',')[3]))\n",
    "train_label_df2 =train_label_df2.drop(['uid'], axis=1)\n",
    "#uid,likes,comments,followers\n",
    "#B-A-bW9l1Gm,601,43,6715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_label_df, train_label_df2], axis=0).reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir('data/') if isfile(join('data/', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>data/B9dZzI1n-EV.jpg</td>\n      <td>0.112025</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>data/B9cQMfVnkMe.jpg</td>\n      <td>0.638448</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>data/B9a0IQWnY9M.jpg</td>\n      <td>0.053981</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>data/B9NnTUrHoNX.jpg</td>\n      <td>0.143885</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>data/B9XuP83gx_j.jpg</td>\n      <td>0.030143</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1429</th>\n      <td>data/B9M_-rNg0Er.jpg</td>\n      <td>0.289023</td>\n    </tr>\n    <tr>\n      <th>1430</th>\n      <td>data/B9WejSrBIsL.jpg</td>\n      <td>0.917219</td>\n    </tr>\n    <tr>\n      <th>1431</th>\n      <td>data/B9wPREWhq0l.jpg</td>\n      <td>0.128363</td>\n    </tr>\n    <tr>\n      <th>1432</th>\n      <td>data/B9Y_fqoHwUn.jpg</td>\n      <td>0.286299</td>\n    </tr>\n    <tr>\n      <th>1433</th>\n      <td>data/B9Y2fYmne_p.jpg</td>\n      <td>0.022442</td>\n    </tr>\n  </tbody>\n</table>\n<p>1434 rows Ã— 2 columns</p>\n</div>",
      "text/plain": "                      path    target\n0     data/B9dZzI1n-EV.jpg  0.112025\n1     data/B9cQMfVnkMe.jpg  0.638448\n2     data/B9a0IQWnY9M.jpg  0.053981\n3     data/B9NnTUrHoNX.jpg  0.143885\n4     data/B9XuP83gx_j.jpg  0.030143\n...                    ...       ...\n1429  data/B9M_-rNg0Er.jpg  0.289023\n1430  data/B9WejSrBIsL.jpg  0.917219\n1431  data/B9wPREWhq0l.jpg  0.128363\n1432  data/B9Y_fqoHwUn.jpg  0.286299\n1433  data/B9Y2fYmne_p.jpg  0.022442\n\n[1434 rows x 2 columns]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = train_df\n",
    "j = 0\n",
    "for i in train_df['path']:\n",
    "    #print(i.split(\"/\")[-1])\n",
    "    if i.split(\"/\")[-1] not in onlyfiles:\n",
    "        new_df = new_df.drop(train_df.index[train_df['path'] == i].tolist())\n",
    "    else:\n",
    "      j+=1\n",
    "train_df = new_df.reset_index().drop(['index'], axis=1)\n",
    "# All are found\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Found 1434 validated image filenames.\n"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=5,\n",
    "    \n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col = 'path',\n",
    "        y_col = 'target',\n",
    "        # directory='data',\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='other',\n",
    "       #  preprocessing_function=lambda x: x print(x)\n",
    ")  # since we use binary_crossentropy loss, we need binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['/device:CPU:0', '/device:XLA_CPU:0']\n"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=12800,\n",
    "        epochs=50,\n",
    "        #validation_data=validation_generator,\n",
    "        #validation_steps=800 // batch_size)\n",
    "        verbose = 2,\n",
    "        # use_multiprocessing=False\n",
    ")\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}